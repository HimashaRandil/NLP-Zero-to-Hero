{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing Small Audio Files"
      ],
      "metadata": {
        "id": "RnPXzJuMWJI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSZ6EabZKtr_",
        "outputId": "74ab140f-72aa-477f-9622-b76dcf8a840b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install SpeechRecognition pydub -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr"
      ],
      "metadata": {
        "id": "-rUcnQTfKyB5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"16-122828-0002.wav\""
      ],
      "metadata": {
        "id": "WsiFP1sPNnwe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the recognizer\n",
        "r = sr.Recognizer()"
      ],
      "metadata": {
        "id": "B-XwYPKOOCQI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with sr.AudioFile(filename) as source:\n",
        "    # listen for the data (load audio to memory)\n",
        "    audio_data = r.record(source)\n",
        "    # recognize (convert from speech to text)\n",
        "    text = r.recognize_google(audio_data)\n",
        "    print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7fn6PPfOVum",
        "outputId": "9fbf43af-081f-4b25-9eb6-09563fd2c7c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I believe you are just talking nonsense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing Large Audio Files"
      ],
      "metadata": {
        "id": "A9JG0oUeOfWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub -qU"
      ],
      "metadata": {
        "id": "Nia49XUyUft2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import speech_recognition as sr\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence"
      ],
      "metadata": {
        "id": "kiZx_D_DOhBE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a speech recognition object\n",
        "r = sr.Recognizer()"
      ],
      "metadata": {
        "id": "LiET5gfkT9UI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(path):\n",
        "    # use the audio file as the audio source\n",
        "    with sr.AudioFile(path) as source:\n",
        "        audio_listened = r.record(source)\n",
        "        # try converting it to text\n",
        "        text = r.recognize_google(audio_listened)\n",
        "    return text"
      ],
      "metadata": {
        "id": "_1i4TmyCUAFC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_large_audio_transcription_on_silence(path):\n",
        "    \"\"\"Splitting the large audio file into chunks\n",
        "    and apply speech recognition on each of these chunks\"\"\"\n",
        "    # open the audio file using pydub\n",
        "    sound = AudioSegment.from_file(path)\n",
        "    # split audio sound where silence is 500 miliseconds or more and get chunks\n",
        "    chunks = split_on_silence(sound,\n",
        "        # experiment with this value for your target audio file\n",
        "        min_silence_len = 500,\n",
        "        # adjust this per requirement\n",
        "        silence_thresh = sound.dBFS-14,\n",
        "        # keep the silence for 1 second, adjustable as well\n",
        "        keep_silence=500,\n",
        "    )\n",
        "    folder_name = \"audio-chunks\"\n",
        "    # create a directory to store the audio chunks\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    whole_text = \"\"\n",
        "    # process each chunk\n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\n",
        "        # export audio chunk and save it in\n",
        "        # the `folder_name` directory.\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "        # recognize the chunk\n",
        "        try:\n",
        "            text = transcribe_audio(chunk_filename)\n",
        "        except sr.UnknownValueError as e:\n",
        "            print(\"Error:\", str(e))\n",
        "        else:\n",
        "            text = f\"{text.capitalize()}. \"\n",
        "            print(chunk_filename, \":\", text)\n",
        "            whole_text += text\n",
        "    # return the text for all chunks detected\n",
        "    return whole_text"
      ],
      "metadata": {
        "id": "mVKMFLGjUZTe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/voice.wav'\n",
        "print(\"\\nFull text:\", get_large_audio_transcription_on_silence(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlHPO8GcUn4B",
        "outputId": "ee467626-079f-49be-b9da-7f59ab006479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio-chunks/chunk1.wav : Dear fellow scholars this is too many papers with calories today we are going to listen to some amazing improvements in the area of ai-based voice cloning. \n",
            "audio-chunks/chunk2.wav : For instance if someone wanted to clone my voice there are hours and hours of my recordings on youtube and elsewhere they could do it with previously existing techniques. \n",
            "audio-chunks/chunk3.wav : But the question today is if we had even more advanced methods to do this. \n",
            "audio-chunks/chunk4.wav : How big of a sound sample would we really need for this. \n",
            "audio-chunks/chunk5.wav : Do we need a few hours. \n",
            "audio-chunks/chunk6.wav : A few minutes. \n",
            "audio-chunks/chunk7.wav : The answer is no. \n",
            "audio-chunks/chunk8.wav : Not at. \n",
            "audio-chunks/chunk9.wav : Hold on to your papers because this new technique only requires 5 seconds. \n",
            "audio-chunks/chunk10.wav : Let's listen to a couple examples. \n",
            "audio-chunks/chunk11.wav : The norsemen considered the rainbow as a bridge over which the gods passed from earth to their home in the sky. \n",
            "audio-chunks/chunk12.wav : Take a look at these pages for crooked creek drive. \n",
            "audio-chunks/chunk13.wav : Terrible listings for gas station. \n",
            "audio-chunks/chunk14.wav : Is the forecast for the next 4 days. \n",
            "audio-chunks/chunk15.wav : Please take the shape of a long round are. \n",
            "audio-chunks/chunk16.wav : What is path high above and it's two lanes apparently beyond the horizon. \n",
            "audio-chunks/chunk17.wav : Take a look at these pages for crooked creek drive. \n",
            "audio-chunks/chunk18.wav : There are several listings for gas station. \n",
            "audio-chunks/chunk19.wav : He is the full cast for the next 4 days. \n",
            "audio-chunks/chunk20.wav : Absolutely incredible the number of the voice is very similar and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. \n",
            "audio-chunks/chunk21.wav : This requires a certain kind of intelligence and quite a bit of that. \n",
            "audio-chunks/chunk22.wav : So while we are at. \n",
            "audio-chunks/chunk23.wav : How does this new system work. \n",
            "audio-chunks/chunk24.wav : Well it requires three components. \n",
            "audio-chunks/chunk25.wav : One the speaker encoder is a neural network that was trained on thousands and thousands of speakers. \n",
            "audio-chunks/chunk26.wav : And is meant to squeeze all this learned data into a compressed representation in other words it tries to learn the essence of human speech from many many speakers. \n",
            "audio-chunks/chunk27.wav : To clarify i will add that this system listens to thousands of people talking to learn the intricacies of human speech. \n",
            "audio-chunks/chunk28.wav : But this training step needs to be done only once and after that it was allowed just 5 seconds of speech data from someone they haven't heard of previously and later the synthesis takes place using this 5 seconds as an input. \n",
            "audio-chunks/chunk29.wav : Two we have a synthesizer that takes text as an input. \n",
            "audio-chunks/chunk30.wav : This is what we would like our test subject to say and it gives us a mass spectrogram which is a concise representation of someone's voice and the intonation the implementation of this module is based on deepmind tacotron 2 technique. \n",
            "audio-chunks/chunk31.wav : And here you can see an example of this mass spectrogram built for a male and two female speakers. \n",
            "audio-chunks/chunk32.wav : On the left we have the spectrograms for the reference recordings the voice samples if you will and on the right with specify a piece of text that we would like the learning algorithm to other and it produces this corresponding synthesized spectrograms. \n",
            "audio-chunks/chunk33.wav : But eventually we would like to listen to something and for that we need a waveform as an output. \n",
            "audio-chunks/chunk34.wav : So the third element is that a neurological order that does exactly that and this component is implemented by deep minds wavenet technique. \n",
            "audio-chunks/chunk35.wav : This is the architecture that led to this amazing examples. \n",
            "audio-chunks/chunk36.wav : So how do we measure exactly how amazing it is. \n",
            "audio-chunks/chunk37.wav : When we have a solution evaluating it is also anything but trivial. \n",
            "audio-chunks/chunk38.wav : In principle we are looking for a result that is both close to the recording that we have of the target person but says something completely different and all this in a natural manner. \n",
            "audio-chunks/chunk39.wav : This naturalness and similarity can be measured but we are not nearly done yet because the problem gets even more difficult. \n",
            "audio-chunks/chunk40.wav : For instance it matters how we fit the three puzzle pieces together and then what date are we trained it on of course also matters a great deal. \n",
            "audio-chunks/chunk41.wav : Here you see that if we train on a one data set and test the results against a different one and then swap the two. \n",
            "audio-chunks/chunk42.wav : And the results in naturalness and similarity will differ significantly. \n",
            "audio-chunks/chunk43.wav : The paper contains a very detailed evaluation section that explains how to deal with these difficult. \n",
            "audio-chunks/chunk44.wav : The mean opinion score is measured in this section which is a number that describes how well a sound sample would pass as genuine human speech. \n",
            "audio-chunks/chunk45.wav : And we haven't even talked about the speaker verification part so make sure to have a look at the paper. \n",
            "audio-chunks/chunk46.wav : So indeed we can clone each other's voice by using a sample of only 5 seconds. \n",
            "audio-chunks/chunk47.wav : What a time to be alive. \n",
            "audio-chunks/chunk48.wav : This episode has been supported by weights and biases. \n",
            "audio-chunks/chunk49.wav : Weights and biases provides tools to track your experiments in your deep learning project. \n",
            "audio-chunks/chunk50.wav : You can save you a ton of time and money in this project it is being used by openai toyota research. \n",
            "audio-chunks/chunk51.wav : Stanford and berkeley. \n",
            "audio-chunks/chunk52.wav : They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly. \n",
            "audio-chunks/chunk53.wav : What are the most common errors you can make and how to fix them. \n",
            "audio-chunks/chunk54.wav : Is really great you got to have a look so make sure to visit them through wendy.com slash papers w a n d b.com slash papers or just click the link in the video description and you can get a free demo today or thanks to weights and biases for helping us make better videos for you thanks for watching and for your generous support and i'll see you next time. \n",
            "\n",
            "Full text: Dear fellow scholars this is too many papers with calories today we are going to listen to some amazing improvements in the area of ai-based voice cloning. For instance if someone wanted to clone my voice there are hours and hours of my recordings on youtube and elsewhere they could do it with previously existing techniques. But the question today is if we had even more advanced methods to do this. How big of a sound sample would we really need for this. Do we need a few hours. A few minutes. The answer is no. Not at. Hold on to your papers because this new technique only requires 5 seconds. Let's listen to a couple examples. The norsemen considered the rainbow as a bridge over which the gods passed from earth to their home in the sky. Take a look at these pages for crooked creek drive. Terrible listings for gas station. Is the forecast for the next 4 days. Please take the shape of a long round are. What is path high above and it's two lanes apparently beyond the horizon. Take a look at these pages for crooked creek drive. There are several listings for gas station. He is the full cast for the next 4 days. Absolutely incredible the number of the voice is very similar and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. This requires a certain kind of intelligence and quite a bit of that. So while we are at. How does this new system work. Well it requires three components. One the speaker encoder is a neural network that was trained on thousands and thousands of speakers. And is meant to squeeze all this learned data into a compressed representation in other words it tries to learn the essence of human speech from many many speakers. To clarify i will add that this system listens to thousands of people talking to learn the intricacies of human speech. But this training step needs to be done only once and after that it was allowed just 5 seconds of speech data from someone they haven't heard of previously and later the synthesis takes place using this 5 seconds as an input. Two we have a synthesizer that takes text as an input. This is what we would like our test subject to say and it gives us a mass spectrogram which is a concise representation of someone's voice and the intonation the implementation of this module is based on deepmind tacotron 2 technique. And here you can see an example of this mass spectrogram built for a male and two female speakers. On the left we have the spectrograms for the reference recordings the voice samples if you will and on the right with specify a piece of text that we would like the learning algorithm to other and it produces this corresponding synthesized spectrograms. But eventually we would like to listen to something and for that we need a waveform as an output. So the third element is that a neurological order that does exactly that and this component is implemented by deep minds wavenet technique. This is the architecture that led to this amazing examples. So how do we measure exactly how amazing it is. When we have a solution evaluating it is also anything but trivial. In principle we are looking for a result that is both close to the recording that we have of the target person but says something completely different and all this in a natural manner. This naturalness and similarity can be measured but we are not nearly done yet because the problem gets even more difficult. For instance it matters how we fit the three puzzle pieces together and then what date are we trained it on of course also matters a great deal. Here you see that if we train on a one data set and test the results against a different one and then swap the two. And the results in naturalness and similarity will differ significantly. The paper contains a very detailed evaluation section that explains how to deal with these difficult. The mean opinion score is measured in this section which is a number that describes how well a sound sample would pass as genuine human speech. And we haven't even talked about the speaker verification part so make sure to have a look at the paper. So indeed we can clone each other's voice by using a sample of only 5 seconds. What a time to be alive. This episode has been supported by weights and biases. Weights and biases provides tools to track your experiments in your deep learning project. You can save you a ton of time and money in this project it is being used by openai toyota research. Stanford and berkeley. They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly. What are the most common errors you can make and how to fix them. Is really great you got to have a look so make sure to visit them through wendy.com slash papers w a n d b.com slash papers or just click the link in the video description and you can get a free demo today or thanks to weights and biases for helping us make better videos for you thanks for watching and for your generous support and i'll see you next time. \n"
          ]
        }
      ]
    }
  ]
}